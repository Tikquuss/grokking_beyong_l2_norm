{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tikquuss/grokking_beyong_l2_norm/blob/main/algorithmic_dataset_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hth1-WPZt8HE"
      },
      "source": [
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vctjRNy4McI6"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Tikquuss/grokking_beyong_l2_norm\n",
        "# %cd grokking_beyong_l2_norm\n",
        "# # #! ls\n",
        "# ! pip install -r requirements.txt\n",
        "LOG_DIR=\"/content/LOGS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9EC-O_tMW8D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(LOG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPSF4lnUuOEV"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neural_nets.mlp import MLP, Encoder_Decoder\n",
        "from neural_nets.data import split_and_create_data_loader\n",
        "from neural_nets.trainer import get_loss, eval_model_classification, run_experiments\n",
        "from neural_nets.checkpointing import get_all_checkpoints\n",
        "from utils.norms import l0_norm_model, l_p_norm_model, nuclear_norm_model\n",
        "from plotters.utils import plot_loss_accs\n",
        "from sparse_recovery.utils import find_memorization_generalization_steps, find_stable_step_final_value, plot_t1_t2\n",
        "from plotters.utils import get_twin_axis, FIGSIZE, LINEWIDTH, FIGSIZE_SMALL, FIGSIZE_LARGE, FIGSIZE_MEDIUM, FONTSIZE, LABEL_FONTSIZE, TICK_LABEL_FONTSIZE, MARKERSIZE"
      ],
      "metadata": {
        "id": "HlQzvdfnvCIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sriWUBEE1kcQ"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ycG_Ki0Kx2Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=97\n",
        "operator = \"+\" # \"+\", \"-\", \"*\", etc\n",
        "r_train=0.4\n",
        "batch_size=2**11\n",
        "eval_batch_size=2**13\n",
        "\n",
        "data = list(itertools.product(range(p), range(p)))\n",
        "X, Y = [], []\n",
        "for x1, x2 in data:\n",
        "    #x = torch.cat([F.one_hot(torch.tensor(x1), num_classes=p), F.one_hot(torch.tensor(x2), num_classes=p)]).float() # (2*p,)\n",
        "    x = torch.stack([F.one_hot(torch.tensor(x1), num_classes=p), F.one_hot(torch.tensor(x2), num_classes=p)]).float() # (2, p)\n",
        "    X.append(x)\n",
        "    Y.append(eval(f\"({x1} {operator} {x2}) % {p}\"))\n",
        "X, Y = torch.stack(X), torch.tensor(Y) # (p^2, 2, p), (p^2,)\n",
        "\n",
        "train_loader, train_loader_for_eval, test_loader = split_and_create_data_loader(\n",
        "    X, Y, r_train=r_train, batch_size=batch_size, eval_batch_size=eval_batch_size, random_state=0, balance=False)"
      ],
      "metadata": {
        "id": "gt8KIRIrv6Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nciLEUaBzH1c"
      },
      "outputs": [],
      "source": [
        "rows, cols = 1, 3\n",
        "figsize=(6, 4)\n",
        "figsize=(8, 6)\n",
        "# figsize=(15, 10)\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "ax = fig.add_subplot(rows, cols, 1)\n",
        "ax.bar(range(p), [(Y==k).sum() for k in range(p)])\n",
        "ax.set_title(f'Class distribution in data', fontsize=26)\n",
        "\n",
        "ax = fig.add_subplot(rows, cols, 2)\n",
        "y_train = train_loader_for_eval.dataset.tensors[1].cpu().numpy() # (N,)\n",
        "ax.bar(range(p), [(y_train==k).sum() for k in range(p)])\n",
        "ax.set_title(f'Class distribution in train data', fontsize=22)\n",
        "\n",
        "ax = fig.add_subplot(rows, cols, 3)\n",
        "y_test = test_loader.dataset.tensors[1].cpu().numpy() # (N,)\n",
        "ax.bar(range(p), [(y_test==k).sum() for k in range(p)])\n",
        "ax.set_title(f'Class distribution in test data', fontsize=22)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40W7VuYh62ke"
      },
      "source": [
        "# $\\beta  h(\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aggregation_mode = 'matrix_product' # 'sum', 'concat', 'matrix_product', 'hadamard_product'\n",
        "embedding_dim = 2**6\n",
        "num_hidden_layers_mlp = 1 # int : number of hidden layer for the mlp (0 for linear model, ...)\n",
        "width_multiplier_mlp = 1. # float : the embedding dimension is multiplied by this number to have the hidden dimension\n",
        "widths_encoder = [p, embedding_dim] # embedding layer\n",
        "widths_decoder = [embedding_dim] + [int(embedding_dim*width_multiplier_mlp)]*num_hidden_layers_mlp + [p] # widths of each hidden layer"
      ],
      "metadata": {
        "id": "jCe7UqmL_HrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {}\n",
        "args['fileName'] = \"mlp_algorithmic_dataset\"\n",
        "args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "os.makedirs(args['exp_dir'], exist_ok=True)\n",
        "\n",
        "args = {}\n",
        "args['fileName'] = \"2layers_nn\"\n",
        "args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "os.makedirs(args['exp_dir'], exist_ok=True)\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args[\"device\"] = device\n",
        "args['train_loader'], args['train_loader_for_eval'], args['test_loader'] = train_loader, train_loader_for_eval, test_loader\n",
        "args['verbose'] = True\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "model = Encoder_Decoder(\n",
        "        aggregation_mode,\n",
        "        widths_encoder,\n",
        "        widths_decoder,\n",
        "        activation_class_encoder=None,\n",
        "        activation_class_decoder=nn.ReLU,\n",
        "        bias_encoder=False,\n",
        "        bias_decoder=False,\n",
        "        bias_classifier=False,\n",
        "        init_params=True,\n",
        "        type_init='normal',\n",
        "        seed=None)\n",
        "\n",
        "args['model'] = model\n",
        "print(model)\n",
        "\n",
        "\n",
        "learning_rate = 5e-3\n",
        "args[\"optimizer\"] = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
        "args[\"criterion\"] = nn.CrossEntropyLoss()\n",
        "#args['get_loss'] = get_loss\n",
        "args[\"eval_model\"] = eval_model_classification\n",
        "args['r_train'] = r_train\n",
        "args[\"get_exp_name_function\"] = lambda args : f\"id={args['exp_id']}-p={p}-rtrain={args['r_train']}\"\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args.update({\n",
        "    \"n_epochs\" : 10**4,\n",
        "    \"eval_first\": 10**2 * 1,\n",
        "    \"eval_period\": 10**1 * 1,\n",
        "    \"print_step\": 10**2 * 1,\n",
        "    \"save_model_step\":10**3 * 1,\n",
        "    \"save_statistic_step\":10**3 * 1,\n",
        "    \"verbose\": True,\n",
        "})\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "# l1, l2, l*\n",
        "args['beta_dic'] = {1 : 0.0, 2 : 1e-6, \"nuc\" : 0.0} # {p : beta_p}\n",
        "\n",
        "#args['get_loss'] = get_loss\n",
        "def get_get_loss(beta_dic):\n",
        "    def get_loss_func(model, batch_x, batch_y, criterion) :\n",
        "        loss, scores = get_loss(model, batch_x, batch_y, criterion)\n",
        "        #loss = torch.norm(scores.squeeze() - batch_y.squeeze())**2\n",
        "\n",
        "        # sum of beta * h(Theta)\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name and param.requires_grad:  # Target weight tensors only\n",
        "                for p, beta_p in beta_dic.items():\n",
        "                    if beta_p!=0: loss = loss + beta_p * torch.norm(param, p=p)\n",
        "\n",
        "        return loss, scores\n",
        "\n",
        "    return get_loss_func\n",
        "\n",
        "args['get_loss'] = get_get_loss(beta_dic=args['beta_dic'])\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args['get_other_metrics']=None\n",
        "def get_other_metrics(model, X, Y, Y_hat, loss):\n",
        "    r = {}\n",
        "    with torch.no_grad():\n",
        "        r[\"l0_norm\"] = l0_norm_model(model, threshold=1e-4, proportion=False, only_weights=True, requires_grad=False)\n",
        "        r[\"l1_norm\"] = l_p_norm_model(model, p=1, only_weights=True, requires_grad=False, concat_first=True).item()\n",
        "        r[\"l2_norm\"] = l_p_norm_model(model, p=2, only_weights=True, requires_grad=False, concat_first=True).item()\n",
        "        r[\"l*_norm\"] = nuclear_norm_model(model, only_weights=True, requires_grad=False).item()\n",
        "    return r\n",
        "args['get_other_metrics'] = get_other_metrics\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args['exp_id'] = None\n",
        "args['seed'] = 42\n",
        "\n",
        "args[\"n_epochs\"] = 10**4 * 1 + 1 #\n",
        "args[\"verbose\"] = True\n",
        "\n",
        "args, model, all_metrics = run_experiments(args)"
      ],
      "metadata": {
        "id": "0t6NxLWCIx8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_models, all_metrics = get_all_checkpoints(checkpoint_path=args['checkpoint_path'], exp_name=args['fileName'], just_files=False)"
      ],
      "metadata": {
        "id": "qZcGm-zX2gwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rb8YeZY0NG2"
      },
      "outputs": [],
      "source": [
        "plot_loss_accs(\n",
        "    all_metrics,\n",
        "    train_test_metrics_names = [\"accuracy\", \"loss\"],\n",
        "    other_metrics_names = [\"l0_norm\", \"l1_norm\", \"l2_norm\", \"l*_norm\"],\n",
        "    multiple_runs=False, log_x=True, log_y=False,\n",
        "    figsize=FIGSIZE, linewidth=LINEWIDTH, fontsize=FONTSIZE,\n",
        "    fileName=None, filePath=None, show=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling wrt $\\alpha \\beta$"
      ],
      "metadata": {
        "id": "8ohpnvUdBk2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_beta_dic = {\n",
        "    1 : [5e-9, 1e-8, 1e-7, 1e-6, 5e-6, 1e-5],\n",
        "    2 : [1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5],\n",
        "    'nuc' : [5e-8, 1e-7, 1e-6, 1e-5, 5e-5, 1e-4]\n",
        "}\n",
        "\n",
        "all_alpha = [1e-3, 1e-2, 1e-1]\n",
        "all_alpha = sorted(all_alpha)"
      ],
      "metadata": {
        "id": "nWiNo_UnAMin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=1 # l_p regularization : 1, 2, 'nuc'\n",
        "all_beta = all_beta_dic[p]\n",
        "n_epochs = 10**4 * 1 + 1"
      ],
      "metadata": {
        "id": "AHZr-W-HFT3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_plot_and_label_dic = {\"loss\":\"Loss\", \"error\":\"Error\", \"accuracy\":\"Accuracy\"}\n",
        "to_plot=\"accuracy\"\n",
        "to_plot_label = to_plot_and_label_dic[to_plot]"
      ],
      "metadata": {
        "id": "EVZ9p7BpRbHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "GUNXSdY2GrNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    args['fileName'] = f\"mlp_algorithmic_dataset_l{p}_alpha={alpha}\"\n",
        "    args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "    os.makedirs(args['exp_dir'], exist_ok=True)\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "        print(f\"alpha = {alpha}, {(i+1)}/{len(all_alpha)}, beta_{p}={beta}, {(j+1)}/{len(all_beta)}\")\n",
        "\n",
        "        args['beta_dic'] = {1 : 0.0, 2 : 0.0, \"nuc\" : 0.0} # {p : beta_p}\n",
        "        args['beta_dic'][p] = beta\n",
        "        args['get_loss'] = get_get_loss(beta_dic=args['beta_dic'])\n",
        "\n",
        "        args['exp_id'] = j\n",
        "        args['seed'] = 42\n",
        "\n",
        "        args[\"n_epochs\"] = n_epochs\n",
        "        args[\"verbose\"] = False\n",
        "\n",
        "        model = Encoder_Decoder(\n",
        "            aggregation_mode,\n",
        "            widths_encoder,\n",
        "            widths_decoder,\n",
        "            activation_class_encoder=None,\n",
        "            activation_class_decoder=nn.ReLU,\n",
        "            bias_encoder=False,\n",
        "            bias_decoder=False,\n",
        "            bias_classifier=False,\n",
        "            init_params=True,\n",
        "            type_init='normal',\n",
        "            seed=None\n",
        "        )\n",
        "\n",
        "        args['model'] = model\n",
        "        learning_rate = alpha\n",
        "        args[\"optimizer\"] = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
        "        args[\"criterion\"] = nn.CrossEntropyLoss()\n",
        "        args, model, all_metrics = run_experiments(args)"
      ],
      "metadata": {
        "id": "ECQmejymCq0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load stats**"
      ],
      "metadata": {
        "id": "Ucxx1WcQGpMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_names = ['train_loss', 'test_loss', 'train_accuracy', 'test_accuracy', 'all_models', 'all_steps', 'l0_norm', 'l1_norm', 'l2_norm', 'l*_norm']\n",
        "all_statistics = {key : {} for key in metrics_names  }\n",
        "\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    args['fileName'] = f\"mlp_algorithmic_dataset_l{p}_alpha={alpha}\"\n",
        "    args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "\n",
        "    for key in metrics_names :\n",
        "        all_statistics[key][alpha] = {}\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "        print(f\"alpha = {alpha}, {(i+1)}/{len(all_alpha)}, beta_{p}={beta}, {(j+1)}/{len(all_beta)}\")\n",
        "\n",
        "        args['exp_id'] = j\n",
        "        exp_name = args['get_exp_name_function'](args)\n",
        "        args['checkpoint_path'] = os.path.join(args['exp_dir'], exp_name)\n",
        "\n",
        "        all_models, statistics = get_all_checkpoints(checkpoint_path=args['checkpoint_path'], exp_name=args['fileName'], just_files=True)\n",
        "\n",
        "        all_statistics['train_loss'][alpha][beta] = statistics['train']['loss']\n",
        "        all_statistics['test_loss'][alpha][beta] = statistics['test']['loss']\n",
        "\n",
        "        all_statistics['train_accuracy'][alpha][beta] = statistics['train']['accuracy']\n",
        "        all_statistics['test_accuracy'][alpha][beta] = statistics['test']['accuracy']\n",
        "\n",
        "        all_statistics['all_models'][alpha][beta] = all_models\n",
        "        for key in ['all_steps', 'l0_norm', 'l1_norm', 'l2_norm', 'l*_norm']:\n",
        "            all_statistics[key][alpha][beta] = statistics[key]"
      ],
      "metadata": {
        "id": "7sit2fVFCvGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 30"
      ],
      "metadata": {
        "id": "6IvnG3dUBg6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L=len(all_alpha)\n",
        "cols = min(3, L)\n",
        "rows = L // cols + 1 * (L % cols != 0)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "log_x=False\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    ax.set_title(f'$\\\\alpha={alpha}$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        elif to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "\n",
        "        ax.plot(all_steps, test_errors, '--', color=colors[j], linewidth=LINEWIDTH)\n",
        "        ax.plot(all_steps, train_errors, '-', label=f'$\\\\beta={beta}$', color=colors[j], linewidth=LINEWIDTH)\n",
        "\n",
        "        # Plot times\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" or to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        # elif to_plot == \"accuracy\" :\n",
        "        #     test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, test_errors, K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(train_errors, test_errors, all_steps, train_threshold=min(train_errors), test_threshold=min(test_errors))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "    if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "    if i%cols==0 : ax.set_ylabel(to_plot_label, fontsize=LABEL_FONTSIZE)\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "    ########### Color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    if i==cols-1: cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE)\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "    ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/mlp_algorithmic_dataset_scaling_alpha_and_beta_{p}\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "5oQpr3ipCx6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = 5\n",
        "rows = len(all_alpha)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "if rows!=1 and cols!=1 :\n",
        "    # flatten\n",
        "    axes = [axes[i][j] for i in range(len(axes)) for j in range(len(axes[0]))]\n",
        "\n",
        "log_x=False\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "k=0\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    #ax = fig.add_subplot(rows, cols, k+1)\n",
        "    ax = axes[k]\n",
        "    k+=1\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    ax.set_title(f'$\\\\alpha={alpha} \\ ({to_plot_label})$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        elif to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "\n",
        "        ax.plot(all_steps, test_errors, '--', color=colors[j], linewidth=LINEWIDTH)\n",
        "        ax.plot(all_steps, train_errors, '-', label=f'$\\\\beta={beta}$', color=colors[j], linewidth=LINEWIDTH)\n",
        "\n",
        "        # Plot times\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, test_errors, K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(train_errors, test_errors, all_steps, train_threshold=min(train_errors), test_threshold=min(test_errors))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "\n",
        "    #if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "    #if i%cols==0 : ax.set_ylabel(to_plot_label, fontsize=LABEL_FONTSIZE)\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "    ########### Color bar\n",
        "    # sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    # import matplotlib.colors as mcolors\n",
        "    # sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    # sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    # cbar = plt.colorbar(sm, ax=ax, location='left', pad=0.2, fraction=0.1, shrink=0.9)\n",
        "    # plt.tight_layout()  # Automatically adjusts layout\n",
        "    # cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE)\n",
        "    # # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    # cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "    ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "    for norm_name, p_label in zip(['l0_norm', 'l1_norm', 'l2_norm', 'l*_norm'], [0, 1, 2, '*']):\n",
        "        #ax = fig.add_subplot(rows, cols, k+1)\n",
        "        ax = axes[k]\n",
        "        k+=1\n",
        "        _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "\n",
        "        ax.set_title(f'$\\\\alpha={alpha} \\ (\\ell_{p_label})$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "        for j, beta in enumerate(all_beta) :\n",
        "\n",
        "            all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "            if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "            ax.plot(all_steps, all_statistics[norm_name][alpha][beta], \"-\", color=colors[j], label=f'$\\\\beta={beta}$', linewidth=LINEWIDTH)\n",
        "\n",
        "             # Plot times\n",
        "            if to_plot == \"loss\" :\n",
        "                test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "            elif to_plot == \"error\" or to_plot==\"accuracy\" :\n",
        "                test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "            # elif to_plot == \"accuracy\" :\n",
        "            #     test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "\n",
        "            # t_2, t_2_index = find_stable_step_final_value(all_steps, test_errors, K=3, tolerance_fraction=0.05, M=2)\n",
        "            t_1, t_2 = find_memorization_generalization_steps(train_errors, test_errors, all_steps, train_threshold=min(test_errors), test_threshold=min(test_errors))\n",
        "            #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "            t = t_2\n",
        "            if t is not None :\n",
        "                ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "                ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "        # if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "        # if i%cols==0 : ax.set_ylabel(f'$\\ell_{p_label}$', fontsize=LABEL_FONTSIZE)\n",
        "        ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "        if log_x : ax.set_xscale('log')\n",
        "        if log_y : ax.set_yscale('log')\n",
        "\n",
        "        if (k-2)%cols==0:\n",
        "            ax.legend(fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "# # Create the ScalarMappable for the color bar\n",
        "# sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "# sm.set_array([])\n",
        "# # Add a single horizontal color bar on top of the figure\n",
        "# cbar = fig.colorbar(sm, ax=axes, location='top', orientation='horizontal', pad=0.1, aspect=50, fraction=0.01, shrink=0.7)\n",
        "# cbar.set_label('$\\\\beta$', fontsize=LABEL_FONTSIZE)\n",
        "# cbar.set_ticks(all_beta)\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/mlp_algorithmic_dataset_scaling_alpha_and_beta_{p}_with_norms\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "S3ad3KU9C2Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 2"
      ],
      "metadata": {
        "id": "WYPYAwfjJlM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_T_max_dic = [None, None, None]\n",
        "#all_T_max_dic = [None, 500, 310]\n",
        "kappa=1.5"
      ],
      "metadata": {
        "id": "ChaxYjw0J993"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L=len(all_alpha)\n",
        "cols = min(3, L)\n",
        "rows = L // cols + 1 * (L % cols != 0)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "log_x=False\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    ax.set_title(f'$\\\\alpha={alpha}$', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        elif to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "\n",
        "        T_max = all_T_max_dic[i]\n",
        "        all_steps, test_errors, train_errors = all_steps[:T_max], test_errors[:T_max], train_errors[:T_max]\n",
        "\n",
        "        ax.plot(all_steps, test_errors, '--', color=colors[j], linewidth=LINEWIDTH*kappa)\n",
        "        ax.plot(all_steps, train_errors, '-', label=f'$\\\\beta={beta}$', color=colors[j], linewidth=LINEWIDTH*kappa)\n",
        "\n",
        "\n",
        "        # Plot times\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" or to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        all_steps, test_errors, train_errors = all_steps[:T_max], test_errors[:T_max], train_errors[:T_max]\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, test_errors, K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(train_errors, test_errors, all_steps, train_threshold=min(train_errors), test_threshold=min(test_errors))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "    if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    if i%cols==0 : ax.set_ylabel(to_plot_label, fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    ########### Color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    if i==cols-1: cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "    cbar.ax.tick_params(labelsize=TICK_LABEL_FONTSIZE*(kappa/2))  #\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "    if i==0 :ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/mlp_algorithmic_dataset_scaling_alpha_and_beta_{p}_small_plot\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "UYR1rwfzJmbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 1"
      ],
      "metadata": {
        "id": "uZW0isb7HwGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# args, LOG_DIR\n",
        "def get_all_statistics_mlp(p, all_alpha, all_beta):\n",
        "    metrics_names = ['train_loss', 'test_loss', 'train_accuracy', 'test_accuracy', 'all_models', 'all_steps', 'l0_norm', 'l1_norm', 'l2_norm', 'l*_norm']\n",
        "    all_statistics = {key : {} for key in metrics_names  }\n",
        "\n",
        "    for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "        args['fileName'] = f\"mlp_algorithmic_dataset_l{p}_alpha={alpha}\"\n",
        "        args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "\n",
        "        for key in metrics_names :\n",
        "            all_statistics[key][alpha] = {}\n",
        "\n",
        "        for j, beta in enumerate(all_beta) :\n",
        "            print(f\"alpha = {alpha}, {(i+1)}/{len(all_alpha)}, beta_{p}={beta}, {(j+1)}/{len(all_beta)}\")\n",
        "\n",
        "            args['exp_id'] = j\n",
        "            exp_name = args['get_exp_name_function'](args)\n",
        "            args['checkpoint_path'] = os.path.join(args['exp_dir'], exp_name)\n",
        "\n",
        "            all_models, statistics = get_all_checkpoints(checkpoint_path=args['checkpoint_path'], exp_name=args['fileName'], just_files=True)\n",
        "\n",
        "            all_statistics['train_loss'][alpha][beta] = statistics['train']['loss']\n",
        "            all_statistics['test_loss'][alpha][beta] = statistics['test']['loss']\n",
        "\n",
        "            all_statistics['train_accuracy'][alpha][beta] = statistics['train']['accuracy']\n",
        "            all_statistics['test_accuracy'][alpha][beta] = statistics['test']['accuracy']\n",
        "\n",
        "            all_statistics['all_models'][alpha][beta] = all_models\n",
        "            for key in ['all_steps', 'l0_norm', 'l1_norm', 'l2_norm', 'l*_norm']:\n",
        "                all_statistics[key][alpha][beta] = statistics[key]\n",
        "\n",
        "    return all_statistics"
      ],
      "metadata": {
        "id": "Ot8p5lsnCHaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_runs = [(1, 0.01), (2, 0.01),  ('nuc', 0.001)] # (p, alpha)\n",
        "selected_runs = [(1, 0.01), (2, 0.01),  ('nuc', 0.01)] # (p, alpha)\n",
        "selected_statistics = {}\n",
        "for p, alpha in selected_runs :\n",
        "    print(f\"p={p}, alpha={alpha}\")\n",
        "    selected_statistics[p] = get_all_statistics_mlp(p, all_alpha=[alpha], all_beta=all_beta_dic[p])"
      ],
      "metadata": {
        "id": "8sKGa5yEIx8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_T_max_dic = {1:700, 2:500, 'nuc':500}\n",
        "kappa=1.5"
      ],
      "metadata": {
        "id": "JKRO_qFQAFsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows, cols = 2, len(selected_runs)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "log_x=False\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "\n",
        "for i, (p, alpha) in enumerate(selected_runs) :\n",
        "    all_statistics = selected_statistics[p]\n",
        "    all_beta = all_beta_dic[p]\n",
        "    T_max = all_T_max_dic[p]\n",
        "\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    #ax.set_title(f'$\\\\alpha={alpha}$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "    ax.set_title(f\"$\\ell_{{{'*' if p=='nuc' else p}}}$ reg.\", fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        elif to_plot == \"accuracy\"  :\n",
        "            test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "\n",
        "        all_steps, test_errors, train_errors = all_steps[:T_max], test_errors[:T_max], train_errors[:T_max]\n",
        "        ax.plot(all_steps, test_errors, '--', color=colors[j], linewidth=LINEWIDTH*kappa)\n",
        "        ax.plot(all_steps, train_errors, '-', label=f'$\\\\beta={beta}$', color=colors[j], linewidth=LINEWIDTH*kappa)\n",
        "\n",
        "        # Plot times\n",
        "\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" or to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        all_steps, test_errors, train_errors = all_steps[:T_max], test_errors[:T_max], train_errors[:T_max]\n",
        "        # elif to_plot == \"accuracy\"  :\n",
        "        #     test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, test_errors, K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(train_errors, test_errors, all_steps, train_threshold=min(train_errors), test_threshold=min(test_errors))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "    #if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    if i%cols==0 : ax.set_ylabel(to_plot_label, fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    ########### Color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    if i==cols-1: cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "    cbar.ax.tick_params(labelsize=TICK_LABEL_FONTSIZE*(kappa/2))  #\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "    if i==0 :ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    #if (k-2)%cols==0:\n",
        "    #ax.legend(fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "#################################################################################\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "p=1\n",
        "all_statistics = selected_statistics[p]\n",
        "all_beta = all_beta_dic[p]\n",
        "T_max = all_T_max_dic[p]\n",
        "for i, (norm_name, p_label) in enumerate(zip(['l1_norm', 'l2_norm', 'l*_norm'], [1, 2, '*'])):\n",
        "\n",
        "    ax = fig.add_subplot(rows, cols, 3+i+1)\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    #ax.set_title(f'$\\\\alpha={alpha}$', fontsize=LABEL_FONTSIZE)\n",
        "    #ax.set_title(f'$\\\\alpha={alpha} \\ (\\ell_{p_label})$', fontsize=LABEL_FONTSIZE)\n",
        "    ax.set_title(f'$\\ell_{p}$ reg., $\\\\ell_{p_label}$ norm', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "\n",
        "        norms = np.array(all_statistics[norm_name][alpha][beta])\n",
        "        norms = norms / 1000 # np.max(norms)\n",
        "        ax.plot(all_steps[:T_max], norms[:T_max], \"-\", color=colors[j], label=f'$\\\\beta={beta}$', linewidth=LINEWIDTH*kappa)\n",
        "\n",
        "\n",
        "        # Plot times\n",
        "        if to_plot == \"loss\" :\n",
        "            test_errors, train_errors = all_statistics['test_loss'][alpha][beta], all_statistics['train_loss'][alpha][beta]\n",
        "        elif to_plot == \"error\" or to_plot == \"accuracy\" :\n",
        "            test_errors, train_errors = 1-np.array(all_statistics['test_accuracy'][alpha][beta]), 1-np.array(all_statistics['train_accuracy'][alpha][beta])\n",
        "        all_steps, test_errors, train_errors = all_steps[:T_max], test_errors[:T_max], train_errors[:T_max]\n",
        "        # elif to_plot == \"accuracy\"  :\n",
        "        #     test_errors, train_errors = all_statistics['test_accuracy'][alpha][beta], all_statistics['train_accuracy'][alpha][beta]\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, test_errors, K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(train_errors, test_errors, all_steps, train_threshold=min(train_errors), test_threshold=min(test_errors))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "    #if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "    ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    if i%cols==0 : ax.set_ylabel('Norm ($\\\\times 10^{-3}$)', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    ########## Color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    if i==cols-1: cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "    cbar.ax.tick_params(labelsize=TICK_LABEL_FONTSIZE*(kappa/2))  #\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    #if log_y : ax.set_yscale('log')\n",
        "\n",
        "    # legend_elements = [\n",
        "    #     Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "    #     Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "    #     ]\n",
        "    # ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "#################################################################################\n",
        "#################################################################################\n",
        "\n",
        "########\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/mlp_algorithmic_dataset_scaling_beta_l1_l2_lnuc\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "CkpSheS_LzVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
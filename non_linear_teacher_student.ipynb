{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tikquuss/grokking_beyong_l2_norm/blob/main/non_linear_teacher_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qdSr1wnaIyVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ..."
      ],
      "metadata": {
        "id": "v-VSpyTZJLYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/Tikquuss/grokking_beyong_l2_norm\n",
        "# %cd grokking_beyong_l2_norm\n",
        "# # #! ls\n",
        "# ! pip install -r requirements.txt\n",
        "LOG_DIR=\"/content/LOGS\""
      ],
      "metadata": {
        "id": "tZtZQF16JIvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(LOG_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "crWMNQGXBDnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'using device: {device}')"
      ],
      "metadata": {
        "id": "IWqiz-SkJVI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neural_nets.mlp import MLP\n",
        "from neural_nets.data import split_and_create_data_loader\n",
        "from neural_nets.trainer import get_loss, eval_model_regression, run_experiments\n",
        "from neural_nets.checkpointing import get_all_checkpoints\n",
        "from utils.norms import l0_norm_model, l_p_norm_model, nuclear_norm_model\n",
        "from plotters.utils import plot_loss_accs\n",
        "from sparse_recovery.utils import find_memorization_generalization_steps, find_stable_step_final_value, plot_t1_t2\n",
        "from plotters.utils import get_twin_axis, FIGSIZE, LINEWIDTH, FIGSIZE_SMALL, FIGSIZE_LARGE, FIGSIZE_MEDIUM, FONTSIZE, LABEL_FONTSIZE, TICK_LABEL_FONTSIZE, MARKERSIZE"
      ],
      "metadata": {
        "id": "gXV1NlPpdice"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = nn.ReLU  # Activations for each layer"
      ],
      "metadata": {
        "id": "UZT6-AxK03lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "a9hr3fwjVGh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 10**2 * 1 # Number of training points\n",
        "N_test = 10**3 * 1 # Number of test points\n",
        "N=N_test+N_train\n",
        "\n",
        "eval_batch_size = 2**13\n",
        "batch_size = 2**13\n",
        "\n",
        "# data_seed = 42 * 2\n",
        "model_seed_teacher = 1234 * 2\n",
        "\n",
        "d = 20 * 5\n",
        "C = 2\n",
        "num_hidden_layers_mlp = 1 # int : number of hidden layer (0 for linear model, ...)\n",
        "width_multiplier_mlp = 5 # float : the input dimension is multiplied by this number to have the hidden dimension\n",
        "widths = [d] + [int(d*width_multiplier_mlp)]*num_hidden_layers_mlp + [C]\n",
        "print(widths)\n",
        "\n",
        "teacher = MLP(widths, activation_class=activation, bias=False, bias_classifier=False, init_params=True, last_layer_zero_init=False, seed=model_seed_teacher)\n",
        "print(teacher)\n",
        "\n",
        "#############################################################################################\n",
        "#############################################################################################\n",
        "\n",
        "variance = 1.0\n",
        "W = teacher.readout[0].weight.data # (h, d)\n",
        "teacher.readout[0].weight.data = np.sqrt(variance) *  torch.randn(W.shape).to(device)\n",
        "\n",
        "V=teacher.fc.weight.data # (C, h)\n",
        "r=V.shape[1]\n",
        "variance = (1/r)**2\n",
        "teacher.fc.weight.data = np.sqrt(variance) *  torch.randn(V.shape).to(device)\n",
        "\n",
        "#############################################################################################\n",
        "#############################################################################################\n",
        "\n",
        "variance = 1.0\n",
        "X = np.sqrt(variance) * np.random.randn(N, d).astype(np.float32)\n",
        "X = torch.tensor(X).to(device)\n",
        "\n",
        "with torch.no_grad() :\n",
        "    Y = teacher(X)\n",
        "\n",
        "#############################################################################################\n",
        "#############################################################################################\n",
        "\n",
        "# Split\n",
        "r_train = N_train/(N_train+N_test) # x100%\n",
        "train_loader, train_loader_for_eval, test_loader = split_and_create_data_loader(\n",
        "    X, Y, r_train=r_train, batch_size=batch_size, eval_batch_size=eval_batch_size, random_state=0, balance=False)"
      ],
      "metadata": {
        "id": "j5M98BrnGXRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\beta  h(\\theta)$"
      ],
      "metadata": {
        "id": "zn7ogD-_c9J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {}\n",
        "args['fileName'] = \"2layers_nn\"\n",
        "args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "os.makedirs(args['exp_dir'], exist_ok=True)\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args[\"device\"] = device\n",
        "args['train_loader'], args['train_loader_for_eval'], args['test_loader'] = train_loader, train_loader_for_eval, test_loader\n",
        "args['verbose'] = True\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args[\"widths\"] = widths\n",
        "# Create the student MLP model\n",
        "model = MLP(args[\"widths\"], activation_class=activation, bias=False, bias_classifier=False, init_params=True, type_init='normal', seed=None)\n",
        "args['model'] = model\n",
        "print(model)\n",
        "\n",
        "\n",
        "learning_rate = 5e-3\n",
        "args[\"optimizer\"] = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
        "args[\"criterion\"] = nn.MSELoss()\n",
        "#args['get_loss'] = get_loss\n",
        "args[\"eval_model\"] = eval_model_regression\n",
        "args[\"get_exp_name_function\"] = lambda args : f\"id={args['exp_id']}-d={d},widths={'-'.join(map(str, args['widths']))}\"\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args.update({\n",
        "    \"n_epochs\" : 10**3 * 1,\n",
        "    \"eval_first\": 10**2,\n",
        "    \"eval_period\": 10**1,\n",
        "    \"print_step\": 10**3,\n",
        "    \"save_model_step\":10**4,\n",
        "    \"save_statistic_step\":10**4,\n",
        "    \"verbose\": True,\n",
        "})\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "# l1, l2, l*\n",
        "args['beta_dic'] = {1 : 0.0, 2 : 1e-6, \"nuc\" : 0.0} # {p : beta_p}\n",
        "\n",
        "#args['get_loss'] = get_loss\n",
        "def get_get_loss(beta_dic):\n",
        "    def get_loss_func(model, batch_x, batch_y, criterion) :\n",
        "        loss, scores = get_loss(model, batch_x, batch_y, criterion)\n",
        "        #loss = torch.norm(scores.squeeze() - batch_y.squeeze())**2\n",
        "\n",
        "        # sum of beta * h(Theta)\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name and param.requires_grad:  # Target weight tensors only\n",
        "                for p, beta_p in beta_dic.items():\n",
        "                    if beta_p!=0: loss = loss + beta_p * torch.norm(param, p=p)\n",
        "\n",
        "        return loss, scores\n",
        "\n",
        "    return get_loss_func\n",
        "\n",
        "args['get_loss'] = get_get_loss(beta_dic=args['beta_dic'])\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args['get_other_metrics']=None\n",
        "def get_other_metrics(model, X, Y, Y_hat, loss):\n",
        "    r = {}\n",
        "    with torch.no_grad():\n",
        "        r[\"l0_norm\"] = l0_norm_model(model, threshold=1e-4, proportion=False, only_weights=True, requires_grad=False)\n",
        "        r[\"l1_norm\"] = l_p_norm_model(model, p=1, only_weights=True, requires_grad=False, concat_first=True).item()\n",
        "        r[\"l2_norm\"] = l_p_norm_model(model, p=2, only_weights=True, requires_grad=False, concat_first=True).item()\n",
        "        r[\"l*_norm\"] = nuclear_norm_model(model, only_weights=True, requires_grad=False).item()\n",
        "    return r\n",
        "args['get_other_metrics'] = get_other_metrics\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "########################################################################################\n",
        "\n",
        "args['exp_id'] = None\n",
        "args['seed'] = 42\n",
        "\n",
        "args[\"n_epochs\"] = 10**5 * 1 + 1 #\n",
        "args[\"verbose\"] = True\n",
        "\n",
        "args, model, all_metrics = run_experiments(args)"
      ],
      "metadata": {
        "id": "Bj7tJRZSkuk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_models, all_metrics = get_all_checkpoints(checkpoint_path=args['checkpoint_path'], exp_name=args['fileName'], just_files=False)"
      ],
      "metadata": {
        "id": "n-KPkYlmMim0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_accs(\n",
        "    all_metrics,\n",
        "    train_test_metrics_names = [\"loss\"],\n",
        "    other_metrics_names = [\"l0_norm\", \"l1_norm\", \"l2_norm\", \"l*_norm\"],\n",
        "    multiple_runs=False, log_x=True, log_y=False,\n",
        "    figsize=FIGSIZE, linewidth=LINEWIDTH, fontsize=FONTSIZE,\n",
        "    fileName=None, filePath=None, show=True)"
      ],
      "metadata": {
        "id": "34eXoOB0ly8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling wrt $\\alpha \\beta$"
      ],
      "metadata": {
        "id": "Erg2hnhM-MPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p=1 # l_p regularization : 1, 2, 'nuc'\n",
        "\n",
        "all_alpha = [1e-4, (1e-4+1e-3)/2, 1e-3, (1e-3+1e-2)/2 , 1e-2, (1e-2+1e-1)/2]\n",
        "all_alpha = sorted(all_alpha)\n",
        "\n",
        "all_beta = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e-1]\n",
        "\n",
        "n_epochs = 10**6 * 1 + 1"
      ],
      "metadata": {
        "id": "VAyHKmfQRDQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "-wLvgKO9dPLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    args['fileName'] = f\"2layers_nn_l{p}_alpha={alpha}\"\n",
        "    args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "    os.makedirs(args['exp_dir'], exist_ok=True)\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "        print(f\"alpha = {alpha}, {(i+1)}/{len(all_alpha)}, beta_{p}={beta}, {(j+1)}/{len(all_beta)}\")\n",
        "\n",
        "        args['beta_dic'] = {1 : 0.0, 2 : 0.0, \"nuc\" : 0.0} # {p : beta_p}\n",
        "        args['beta_dic'][p] = beta\n",
        "        args['get_loss'] = get_get_loss(beta_dic=args['beta_dic'])\n",
        "\n",
        "        args['exp_id'] = j\n",
        "        args['seed'] = 42\n",
        "\n",
        "        args[\"n_epochs\"] = n_epochs\n",
        "        args[\"verbose\"] = False\n",
        "\n",
        "        model = MLP(args[\"widths\"], activation_class=activation, bias=False, bias_classifier=False, init_params=True, type_init='normal', seed=None)\n",
        "        args['model'] = model\n",
        "        learning_rate = alpha\n",
        "        args[\"optimizer\"] = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
        "        args[\"criterion\"] = nn.MSELoss()\n",
        "\n",
        "        args, model, all_metrics = run_experiments(args)"
      ],
      "metadata": {
        "id": "DKyTMX2ETLdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load stats**"
      ],
      "metadata": {
        "id": "dew0eKJEdSJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_names = ['train_loss', 'test_loss', 'all_models', 'all_steps', 'l0_norm', 'l1_norm', 'l2_norm', 'l*_norm']\n",
        "all_statistics = {key : {} for key in metrics_names  }\n",
        "\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    args['fileName'] = f\"2layers_nn_l{p}_alpha={alpha}\"\n",
        "    args['exp_dir'] = f\"{LOG_DIR}/{args['fileName']}\"\n",
        "\n",
        "    for key in metrics_names :\n",
        "        all_statistics[key][alpha] = {}\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "        print(f\"alpha = {alpha}, {(i+1)}/{len(all_alpha)}, beta_{p}={beta}, {(j+1)}/{len(all_beta)}\")\n",
        "\n",
        "        args['exp_id'] = j\n",
        "        exp_name = args['get_exp_name_function'](args)\n",
        "        args['checkpoint_path'] = os.path.join(args['exp_dir'], exp_name)\n",
        "\n",
        "        all_models, statistics = get_all_checkpoints(checkpoint_path=args['checkpoint_path'], exp_name=args['fileName'], just_files=True)\n",
        "\n",
        "        all_statistics['train_loss'][alpha][beta] = statistics['train']['loss']\n",
        "        all_statistics['test_loss'][alpha][beta] = statistics['test']['loss']\n",
        "        all_statistics['all_models'][alpha][beta] = all_models\n",
        "        for key in ['all_steps', 'l0_norm', 'l1_norm', 'l2_norm', 'l*_norm']:\n",
        "            all_statistics[key][alpha][beta] = statistics[key]"
      ],
      "metadata": {
        "id": "b53sGdF9YrkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figures 31 ($\\ell_1$), 32 ($\\ell_2$) and 33 ($\\ell_*$)"
      ],
      "metadata": {
        "id": "LcRAcXYZSDNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L=len(all_alpha)\n",
        "cols = min(3, L)\n",
        "rows = L // cols + 1 * (L % cols != 0)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "log_x=True\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    ax.set_title(f'$\\\\alpha={alpha}$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "        ax.plot(all_steps, all_statistics['test_loss'][alpha][beta], '--', color=colors[j], linewidth=LINEWIDTH)\n",
        "        ax.plot(all_steps, all_statistics['train_loss'][alpha][beta], '-', label=f'$\\\\beta={beta}$', color=colors[j], linewidth=LINEWIDTH)\n",
        "\n",
        "        # Plot times\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, all_statistics['test_loss'][alpha][beta], K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(\n",
        "            all_statistics['train_loss'][alpha][beta], all_statistics['test_loss'][alpha][beta], all_steps,\n",
        "            train_threshold=min(all_statistics['train_loss'][alpha][beta]), test_threshold=min(all_statistics['test_loss'][alpha][beta]))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "    if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "    if i%cols==0 : ax.set_ylabel('Loss', fontsize=LABEL_FONTSIZE)\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "    ########### Color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE)\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "    ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/2layers_nn_scaling_alpha_and_beta_{p}\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "y2PEdKf2Xm9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = 5\n",
        "rows = len(all_alpha)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "if rows!=1 and cols!=1 :\n",
        "    # flatten\n",
        "    axes = [axes[i][j] for i in range(len(axes)) for j in range(len(axes[0]))]\n",
        "\n",
        "log_x=True\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "k=0\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    #ax = fig.add_subplot(rows, cols, k+1)\n",
        "    ax = axes[k]\n",
        "    k+=1\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    ax.set_title(f'$\\\\alpha={alpha} \\ (Loss)$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "        ax.plot(all_steps, all_statistics['test_loss'][alpha][beta], '--', color=colors[j], linewidth=LINEWIDTH)\n",
        "        ax.plot(all_steps, all_statistics['train_loss'][alpha][beta], '-', label=f'$\\\\beta={beta}$', color=colors[j], linewidth=LINEWIDTH)\n",
        "\n",
        "        # Plot times\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, all_statistics['test_loss'][alpha][beta], K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(\n",
        "            all_statistics['train_loss'][alpha][beta], all_statistics['test_loss'][alpha][beta], all_steps,\n",
        "            train_threshold=min(all_statistics['train_loss'][alpha][beta]), test_threshold=min(all_statistics['test_loss'][alpha][beta]))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "\n",
        "    #if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "    #if i%cols==0 : ax.set_ylabel('Loss', fontsize=LABEL_FONTSIZE)\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "    ########### Color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax, location='left', pad=0.2, fraction=0.1, shrink=0.9)\n",
        "    plt.tight_layout()  # Automatically adjusts layout\n",
        "    cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE)\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "    ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "    for norm_name, p_label in zip(['l0_norm', 'l1_norm', 'l2_norm', 'l*_norm'], [0, 1, 2, '*']):\n",
        "        #ax = fig.add_subplot(rows, cols, k+1)\n",
        "        ax = axes[k]\n",
        "        k+=1\n",
        "        _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "\n",
        "        ax.set_title(f'$\\\\alpha={alpha} \\ (\\ell_{p_label})$', fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "        for j, beta in enumerate(all_beta) :\n",
        "\n",
        "            all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "            if log_x : all_steps = np.array(all_steps) + 1\n",
        "\n",
        "            ax.plot(all_steps, all_statistics[norm_name][alpha][beta], \"-\", color=colors[j], label=f'$\\\\beta={beta}$', linewidth=LINEWIDTH)\n",
        "\n",
        "            # Plot times\n",
        "            # t_2, t_2_index = find_stable_step_final_value(all_steps, all_statistics['test_loss'][alpha][beta], K=3, tolerance_fraction=0.05, M=2)\n",
        "            t_1, t_2 = find_memorization_generalization_steps(\n",
        "                all_statistics['train_loss'][alpha][beta], all_statistics['test_loss'][alpha][beta], all_steps,\n",
        "                train_threshold=min(all_statistics['train_loss'][alpha][beta]), test_threshold=min(all_statistics['test_loss'][alpha][beta]))\n",
        "            #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "            t = t_2\n",
        "            if t is not None :\n",
        "                ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "                ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "        # if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE)\n",
        "        # if i%cols==0 : ax.set_ylabel(f'$\\ell_{p_label}$', fontsize=LABEL_FONTSIZE)\n",
        "        ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE)\n",
        "\n",
        "        if log_x : ax.set_xscale('log')\n",
        "        if log_y : ax.set_yscale('log')\n",
        "\n",
        "        if (k-2)%cols==0:\n",
        "            ax.legend(fontsize=LABEL_FONTSIZE*0.8)\n",
        "\n",
        "# # Create the ScalarMappable for the color bar\n",
        "# sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "# sm.set_array([])\n",
        "# # Add a single horizontal color bar on top of the figure\n",
        "# cbar = fig.colorbar(sm, ax=axes, location='top', orientation='horizontal', pad=0.1, aspect=50, fraction=0.01, shrink=0.7)\n",
        "# cbar.set_label('$\\\\beta$', fontsize=LABEL_FONTSIZE)\n",
        "# cbar.set_ticks(all_beta)\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/2layers_nn_scaling_alpha_and_beta_{p}_with_norms\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "oJ2r-XH37Si8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 10"
      ],
      "metadata": {
        "id": "5vN4_S5c_RQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_alpha = [1e-4, (1e-4+1e-3)/2, 1e-3, (1e-3+1e-2)/2 , 1e-2, (1e-2+1e-1)/2]\n",
        "all_alpha = sorted(all_alpha)\n",
        "all_alpha = [all_alpha[0], all_alpha[2], all_alpha[3]]"
      ],
      "metadata": {
        "id": "aDWneyiI_s9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kappa=1.5"
      ],
      "metadata": {
        "id": "TtagyHIuAHNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L=len(all_alpha)\n",
        "cols = min(3, L)\n",
        "rows = L // cols + 1 * (L % cols != 0)\n",
        "\n",
        "figsize=FIGSIZE_SMALL\n",
        "figsize=(cols*figsize[0], rows*figsize[1])\n",
        "fig = plt.figure(figsize=figsize)\n",
        "\n",
        "log_x=True\n",
        "log_y=False\n",
        "\n",
        "color_indices = np.linspace(0, 1, len(all_beta)+1*0)\n",
        "colors = plt.cm.viridis(color_indices)\n",
        "\n",
        "for i, alpha in enumerate(all_alpha):\n",
        "\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    _, ax, _ = get_twin_axis(ax=ax, no_twin=True)\n",
        "    #_, ax, ax1 = get_twin_axis(ax=ax, no_twin=False)\n",
        "\n",
        "    ax.set_title(f'$\\\\alpha={alpha}$', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    for j, beta in enumerate(all_beta) :\n",
        "\n",
        "        all_steps = all_statistics['all_steps'][alpha][beta]\n",
        "        if log_x : all_steps = np.array(all_steps) + 1\n",
        "        ax.plot(all_steps, all_statistics['test_loss'][alpha][beta], '--', color=colors[j], linewidth=LINEWIDTH*kappa)\n",
        "        ax.plot(all_steps, all_statistics['train_loss'][alpha][beta], '-', label=f'$\\\\beta_{p}={beta}$', color=colors[j], linewidth=LINEWIDTH*kappa)\n",
        "\n",
        "        # Plot times\n",
        "        # t_2, t_2_index = find_stable_step_final_value(all_steps, all_statistics['test_loss'][alpha][beta], K=3, tolerance_fraction=0.05, M=2)\n",
        "        t_1, t_2 = find_memorization_generalization_steps(\n",
        "            all_statistics['train_loss'][alpha][beta], all_statistics['test_loss'][alpha][beta], all_steps,\n",
        "            train_threshold=min(all_statistics['train_loss'][alpha][beta]), test_threshold=min(all_statistics['test_loss'][alpha][beta]))\n",
        "        #plot_t1_t2(ax, t_1, t_2, log_x, log_y, plot_Delta=True)\n",
        "        t = t_2\n",
        "        if t is not None :\n",
        "            ax.axvline(x=t, ymin=0.01, ymax=1., color=colors[j], linestyle='--', lw=1.)\n",
        "            ax.plot([t, t], [0, 0], 'o', color='b')\n",
        "\n",
        "    if (rows-1)*cols <= i < rows*cols : ax.set_xlabel('Steps (t)', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    if i%cols==0 : ax.set_ylabel('Loss', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    ax.tick_params(axis='both', labelsize=TICK_LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "    ########### Color bar\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    import matplotlib.colors as mcolors\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=mcolors.LogNorm(vmin=min(all_beta), vmax=max(all_beta)))\n",
        "    sm.set_array([])  # We only need the colormap here, no actual data\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    if i==cols-1:cbar.set_label(f'$\\\\beta$', fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "    # # Set the ticks to correspond to the values in `all_beta_1`\n",
        "    cbar.set_ticks(all_beta)  # Sets tick positions based on `all_beta`\n",
        "    # cbar.set_ticklabels([str(beta) for beta in all_beta])  # Sets tick labels to match `all_beta`\n",
        "    cbar.ax.tick_params(labelsize=TICK_LABEL_FONTSIZE*(kappa/2))  #\n",
        "\n",
        "    if log_x : ax.set_xscale('log')\n",
        "    if log_y : ax.set_yscale('log')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', linestyle='-', label='Train'),\n",
        "        Line2D([0], [0], color='k', linestyle='--', label='Test')\n",
        "        ]\n",
        "\n",
        "    if i==0 :ax.legend(handles=legend_elements, fontsize=LABEL_FONTSIZE*(3*kappa/4))\n",
        "\n",
        "\n",
        "## Adjust layout and add padding\n",
        "fig.tight_layout(pad=2)  # Adjust padding between plots\n",
        "plt.subplots_adjust(right=0.85)  # Adjust right boundary of the plot to fit color bar\n",
        "\n",
        "##\n",
        "#plt.savefig(f\"{LOG_DIR}/2layers_nn_scaling_alpha_and_beta_{p}_small_plot\"  + '.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "bgXz5Tq6_QhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}